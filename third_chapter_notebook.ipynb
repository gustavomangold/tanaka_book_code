{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cce916",
   "metadata": {},
   "source": [
    "# Capítulo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f64be83",
   "metadata": {},
   "source": [
    "## Secão 3.1\n",
    "Nesta seção, se introduz o conceito de redes neurais utilizando mecânica estatística. \n",
    "\n",
    "### Subseção 3.1.1\n",
    "\n",
    "Em um primeiro exemplo de rede com somente dois *labels* de classificação e aprendizado supervisionado, a função de 'chute' $Q_J$ se constrói por meio da distribuição de Boltzmann e com o uso de um hamiltoniano escrito na forma $H_{J,x}(d) = -(xJ_x+yJ_y+J)d$. Temos:\n",
    "\n",
    "$Q_j(d|x) = \\frac{e^{(xJ_x+yJ_y+J)d}}{1+e^{xJ_x+yJ_y+J}}$\n",
    "\n",
    "onde as constantes $J$ são os parâmetros, que chamamos de *constantes de acoplamento*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e862191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f655f314850>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKeklEQVR4nO3dfXBc1Z0n/G+rGb+ALW3Uko1xN9i4PJXKMAk1EEjIOpELb8hUUmUQtnnbHUxYJzwYcNvEBFgXRluwTjkOEnEgvGQH2HEEfpFAk8yEBFMy+NmEChsmT1WSIg9QNkhtY8v2jGSYIEetu38crnW7+97b59x77j23u7+fqouw1N339Ov59Tm/8zspy7IsEBERERnQZLoBRERE1LgYiBAREZExDESIiIjIGAYiREREZAwDESIiIjKGgQgREREZw0CEiIiIjGEgQkRERMacYboBfiYnJ3Ho0CHMnj0bqVTKdHOIiIhIgmVZOHnyJM455xw0NfmPeSQ6EDl06BByuZzpZhAREVEAQ0NDyGazvpdJdCAye/ZsAOKONDc3G24NERERyRgbG0Mulzvdj/tJdCBiT8c0NzczECEiIqoxMmkVTFYlIiIiYxiIEBERkTEMRIiIiMgYBiJERERkDAMRIiIiMoaBCBERERnDQISIiIiMYSBCRERExjAQISIiImMYiBAREZExDESIiIjIGAYiREREZAwDESIiIjKGgQgREREZw0CEiIiIjGEgQkRERMYwECEiIiJjGIgQERGRMQxEiIiIyBgGIkRERGQMAxEiIiIyhoEIERERGcNAhIiIiIxhIEJERETGMBAhIiIiYxiIEBERkTFnmG4AEVEjKxaB/fuBw4eBefOAJUuAdNp0q4jiw0CEiMiQ/n5g3TpgeHjqd9ks8PDDQGenuXYRxYlTM0REBvT3AytWlAYhAFAoiN/395tpF1HcYgtEvvOd7yCVSiGfz8d1SiKiRCoWxUiIZVX+zf5dPi8uR1TvYglEXn/9dTz++OP49Kc/HcfpiIgSbf/+ypEQJ8sChobE5YjqXeSByAcffIAbbrgBTz75JD7xiU9EfToiosQ7fFjv5YhqWeSByNq1a/HVr34Vy5Yti/pUREQ1Yd48vZcjqmWRrpp57rnn8MYbb+D111+Xuvz4+DjGx8dP/3tsbCyqphERGbNkiVgdUyi454mkUuLvS5bE3zaiuEU2IjI0NIR169bhxz/+MWbMmCF1nS1btqClpeX0kcvlomoeEZEx6bRYoguIoMPJ/ndPD+uJUGNIWZZbPB7eCy+8gKuuugppxzupWCwilUqhqakJ4+PjJX8D3EdEcrkcRkdH0dzcHEUziYiMcasjksuJIIR1RKiWjY2NoaWlRar/jiwQOXnyJN59992S391000345Cc/iW9/+9u44IILqt6Gyh0hIqpFrKxK9Uil/44sR2T27NkVwcZZZ52FTCYjFYQQETWCdBro6DDdCiJzWFmViIiIjIl1r5l9+/bFeToiIiJKOI6IEBERkTEMRIiIiMgYBiJERERkDAMRIiIiMoaBCBERERnDQISIiIiMYSBCRERExjAQISIiImMYiBAREZExDESIiIjIGAYiREREZAwDESIiIjKGgQgREREZw0CEiIiIjGEgQkRERMYwECEiIiJjGIgQERGRMQxEiIiIyJgzTDeAiCjJikVg/37g8GFg3jxgyRIgnTbdKqL6wUCEiMhDfz+wbh0wPDz1u2wWePhhoLPTXLuI6gmnZoiIXPT3AytWlAYhAFAoiN/395tpF1G9YSBCRFSmWBQjIZZV+Tf7d/m8uBwRhcNAhIiozP79lSMhTpYFDA2JyxFROAxEiIjKHD6s93JE5I2BCBFRmXnz9F6OiLwxECEiKrNkiVgdk0q5/z2VAnI5cTkiCoeBCBFRmXRaLNEFKoMR+989PawnQqQDAxEiIhedncCePcD8+aW/z2bF71lHhEgPFjQjigpLcta8zk5g+XI+jURRYiBCFAWW5Kwb6TTQ0WG6FUT1i1MzRLqxJCcRkTQGIkQ6sSQnEZESBiJEOrEkJxGREuaIEOkUV0lOJsISUZ1gIEKkUxwlOZkIS0R1hFMzRDpFXZKzVhJhi0Vg3z7g2WfFT+bEEJEHBiJEOkVZkrNWEmH7+4EFC4ClS4Hrrxc/FyxITpBERInCQIRIt6hKctZCImytjNgQUWIwR4QoClGU5Ez63vTVRmxSKTFis3w5E2vrCPOmKSwGIkRR0V2SM+l706uM2LBUaV1g3jTpwKkZolqR9L3pkz5iQ1pxFo50YSBCVCuSvjd90kdsSJtayZum2sBAhKiWJHlv+qSP2JA2tZA3TbWDOSJEtcYrERYAXn5Z1O0ARB5GR0d8IyT2iM2KFSLocH5dTsKIDWnDWTjSiYEIkUlBlxyUJ8L29wPf+AZw/PjU7x54AMhkgCeeiG+kxB6xcctg7OlpyAzGelxVwlk40illWW6zfMkwNjaGlpYWjI6Oorm52XRziPTSteSgvx+4+mr/y/T1xRsE1GPvG0C9riopFkWNukLBPU8klRL388CBhnzaCWr9NwMRIhPsJQflbz97CkM236NYBM47T/QIfrJZ4OBB9gox0vUUJ5V9/wD3Wbhav38UDgMRoiSzv056ZfupfJ3ct0+UUJcxOJjo+h32IEqhAIyMAO3tIie3FgdTdD7FSeY24pPLNewsHDmo9N/MESGKm87CXyrZgAnOHHTr0Gy1OJXRKLXdoiggTI2HgQhR3HQuOVDJBkxo5qDXFIZteFj8vZaG+htpVYnuAsLUeFhHhChuOpccLFlSWVPETTabyPodfoWxytVSgSyuKiGSx0CEKG66Cn/ZSRUrV1Y/58MPJ3K8vNoUhq3WCmSxthuRPAYiRHHTUaq9v19kQy5dKi4LAE0ub+dMJv6luwpUpyZqZSoj6dX4iZKEgQiRCWFKtXvtNmbPb6xYAWzaBOzdCxw5ktggBFCfmqilqYwkV+MnShIu3yUySbXwV52tC612d8rt2iU3E5UkrO1GjYh1RIjqgVsPtn+/XN2Q7m7g9ttrosertmrGKZdTi7EYBBCZodJ/c2qGKImcOSDXXy9+LlgADAzIXX/9enH5/v4IG6mHPYXR1lb9sioJq14PYQ08JEQNhYEIUdJ45YAUClOJqTIKBXE7NdDzdnbK3zWZhFW/h7BGHhKihsFAhChJ/Apr2L9Lp73XhbpdvkYKcMiUQwGqJ6zKPIQ18pAQNQQGIkRJIlNYo1gUPapsMGLPZxSLYm+aZ58VP8t74mp/j5iu2hsq5dWJyDwGIkRJIlsoI5+XH0IARG6JX8JEAhIqdNXeaJTy6objRiJtGIgQJYlsoYzly4GDB8XqGBk9Pd4JE3fdlZiECh21NxqhvHoC4kYibbh8lyhJ7MIahYJ7kkN5nRCZyzc1+X9dtm/HjaG6JGGW3VZ7SACgtVXUJOnoqL3lvF7Lne1RIxZLoyRIzPLdLVu24LOf/Sxmz56NOXPm4Morr8Qf//jHKE9JVNtU5yeqXd6yqo/Z+/3dUEKFvaPrddepBwt+D4ntxAlg2bLgowimpkWYiEv1KNJA5JVXXsHatWvx2muv4aWXXsKf//xnfPnLX8aHH34Y5WmJapvq/ITf5fN5PW2qgYQKZ3Bgj3hUS6MZHlaffTI5LcJEXKpHsU7NjIyMYM6cOXjllVfwxS9+serlOTVDDS1I+Xfn5S+7DHj0UVHcLKzBQTE0kVD9/WKkwNlJZ7PANdcATz8NHD/uf33Ziq1xTYt4PfXPPiuCn2p6e8VoEpEpKv33GTG1CQAwOjoKAGhtbY3ztERqklIX3J6fCHL5/n5g0SK5TVzSaWBy0j8nJcH71XsFB8PDwPe+J3cb9iiC38NdbVoklRIDUMuXh3u5eAVVDz/cGIm41HhiWzUzOTmJfD6PL3zhC7jgggtcLzM+Po6xsbGSgyhW9bAcwausaLlUShwbNkz9u/zvwFTJ0wSuFfULDlQVCv5/j2NapFpF2JERPbVWiJIktkBk7dq1+N3vfofnnnvO8zJbtmxBS0vL6SOXy8XVPKL6qAuu0jPbOSdbt/rnpACJDc5k6r/JGhnx/3vU9UmqjbhYFnDLLcDNN4vfham1QpQkseSI3HbbbRgYGMCrr76KhQsXel5ufHwc4+Pjp/89NjaGXC7HHBGKXrX96ONaxhp2WmjfvuC78zrPPWeO+N1Pf+q+CUxEa0VV775szoSMHTuAG27w/rvsQxs0nUb29gEgkxE/nbkvuZx4qrh0l5IgMTkilmXh9ttvx/PPP499+/b5BiEAMH36dEyfPj3KJhG5Uxl3jypp0y85QLZ3qTa/YGtvr+zh7RyT/n5g9erqj4czKQIInVcT5O7rzIXwWmFjB0eFgtgh+Ngx98uFTadRGUk5cUI8BV1dwOLFZlOZiMKKNBBZu3Ytent7MTAwgNmzZ+P9998HALS0tGDmzJlRnppIjem64F4Zl/a0kNvIg9vwQbX5BZt9ufLbOHYMWLVKbmrHDs4efBB48slQAVSQuw9M7U/jV7xMhldehVtw5KZ8WkR2ZMd5uSNH5Ntrx4E/+lHsteaI9LMiBMD1eOqpp6SuPzo6agGwRkdHo2wmkWUNDtrT8P7H4KD+c09MWFY2633OVMqycjlxOVtfX+V1slnLuuMOufuxY4f7baTTctevdqRS4ujrE+0eHLSs3l7x03k/JO4+YFnt7ZY1Pu7+8PX1TZ0uTDO9blfmdnK5qdvwemp27Sp9GHbt0vPwR/GSJApLpf+ONBAJi4EIxcbuDb16HrdgQBfVIMirh1Tpibu6gvXcqr18JmNZ8+eX/r6tzbLy+dNBiezdb2tzDxjsh8St87/mGstqbZULINxeDn53rb1dxHPO2EoleNF19PZqf0UShabSf8daR4Qosey64CtWTJVGt0W9HEFlWkimmEW1vWUA4Pvfd78NnSzLvZLYsWPisezpwUftWbx98cMAqk/hHDsGXH01sHu3eJqcOjtFqoo9zfHWW8ATTwA7d05dprVV5OcuWQIcPVo5ZVI+TVItRWZkROSV2ClDOpcS+23/U441Q6jWcfddIpuOrV+DUKlSJZNUK9ODVSs1GpNpIwV8/WcrcBXklwJfe60IRsrZubbTpwP331+Zt/uv/wr89/8OjI5W7mFTXj5GthitM4bUuZS4WBTF2PxqP9ZizRBTe/RQsjEQIXLq7AQOHhRrMHt7xc8DB6JdE2lnXMpUqaqBPV9UNEEMH/QgjybI9UrFosindStjEmRTONn6b26cMaTup2bePJEDbNedc6rFmiH1UCuQosFAhKhcmK1fg55PdsfdOhyHb4KFczGEJVArSeq2y6xq9dMw0ynlS3V1PzXz5pkbpNOtHmoFUnQYiBAlgWyPIzN60t4ebVsjMg9qQwpu5dRVV2GHmU5Zs6Y0Rl2yZKrQWBjlUy4mBumC8Jp2CTJKRY2FyapESVGecelWgEImqdbecderuEYqJZIPTpwQ/446aVXSYagPKdgBhZ1o+oc/yF3vD38QnaVs/Tc3ixcHv64XrykX1f0P4+ZXjK611XytQEo2BiJESSLT49ijJ26f/HaN76Ym72DFsoA77hDZmzt2eJcKjckkUhhGFvuhnnU5b5580TGnBx4QR1ub8ilLzu20f3/4HGDnU1grqhWjW7dO7nbqLP2JFDAQITPC7qmSBCbvg9voyWWXAb/8pRgbnzdPrF3dsKG0h7aXYWzePPW7lhaxlMSASYghgDx6MIn06TLp27aJhEav4Xr7ciqFYN0EicFSKRHAFApiVMV+2oN0pJs3i7izVt8GMqvJf/xjuduqw/QnkhVDXZPAWNCshvlV0/SqPuVVrSqJknYfvNqze7d4/HfssKybbvKuzhVnBS7H8S5y1lXoO90MZ5XTXbu8m5tKuVcmNXHYT7tsYTbn9aKojxcnlWJ0JmoFkjmsrEpm+XXSflVBvWptl6tSMjxyOu5DGOX3f/du//Zs3FhZ3dTw8f9/LW+taBu0mjBx+tduVU7dXkr25VQ7/qiO8sBIJq6L42USh95euccon3cvwx/XW4bix0CEzKnWSWcy/p/O1b4amR6JCLIvjE5R7g8TxVHeNke0US2etP++Y4dldXdXllOX7QRlj3XrRNn28pdWV5c4d1tb9afdjgn9ghGvsvK1SGV3Ar+gkuoPAxEyQ2b3MtlPLTemRyIsy+zmeCY2MtFxdHcrj17JxJu6R0T27vUOjsJ2uO3tJdvrJJrKgKPqFk2mBzMpPgxEyAxdPYPbLl6mRyJssl/Dde9EpivIM3F8/vNKPY9svDkxoXfGyW9gTfVpr9UON8iAo9fux5x2aWwq/TcLmpE+utbfuaXPq5bMjIrKvjA66dzIJG6/+pV0TW+V4lfpNPCNb+hrpl+Vzzlz5G7DftrjLs6rQ9Dqp/VS/ZXMYSBC+oTtfP128VItmRkVlX1hdKqXIgtVejXVeFOlqFgm47+JXHmgY+vvB1av9r/tWtyAzils9dNaqf5KycRAhPSR6aTtGtiqu3iZGokop7IvjE71UmShSq8mW+nUjstkH5bubuDIEWDXrurNcwY6MhviqTzt1XafNbU7rY4Bx1ocBaKEiGGqKDDmiNQgmQnjIOnzqllxcdzPOJcATEz4rziK69CZLFuW0NvXV7lqxevYu3fqYVF5WeTzcrefz8un5bS2ipU11V561fIvTC4IM5X6RPWLyapklkwnHSSbL2lZcVFmJJbf9vi42UDEXnqtMzvU0aupLgiaP190/r294qfMy6KvT/7229pEsKNyd/yChmoJuBs3ml0QZnIxGNUnBiJkXlSddCMUI3C7j35FLOI8brxR32193KvpWBCUyVTGae3toq6HfQ7VGGrFCrXLewUNMgu+/ErBxDHYl7QBR6p9Kv13yrIsy+TUkJ+xsTG0tLRgdHQUzc3NpptDSVEP+9R48dpBLCnOOgv48MNwt2FvFHPgAJBOY98+saBGh9mzgZMnp/5t7wD7u9+Vbq8TlbK7BgDa7t/gYLS709ovPaD05WfnwHAFDKlQ6b+ZrEq1p16z4vyWLoTV3q7ndsIGIYC4f1dfLYLJYlHrgiBnEAKI5Nerrw4ehKi+tCyrMqlT1/2LeuEUl+GSKdx9lygpdNUKyWaBhx4SwYdzZ95zzgm/T72M5mbg0UeBd94BnniidClMOi0Crp4ecWSz+Os1DwOIppcLG9PZq1ZSKbXbcgYNuhY8xbFwavlysRnzvn3i3x0d9RXrUzJxRIQoKXR95b38cmBiQvz/qlXx9yR33gnccANw333Au++KOYV8XvytfD1qoYC/un8FVjd7FzkzLZ+vHCWoxhk0yKxqb6rySZzJRF+jpL9f1Jtbtgx44AFxrF4NDAxEe14i5ogQJYXOZAmbnSTR2qr/tt1kMqJghzPwKRZFD+cx2mMhheMzs5j7pwOYRPK+eg8OiiBg/34xuJPPA8eOuV/WLUcE8M6/sM2aBXzwgXcb3B5WnbxSk5gfQkExR4QoTrqqUFX76hyEXclU99darzb+8IeVvWWVKacULLT9aQhLEHF5fkXOaql2WtINNwCPPy7+5lXP7nvfE3fZ+XKw8y/cKrs2N/sHIYCYUYtq94KwVVWJwmIgQhSGPZ69dKn0fiolnEHM/v3Atm3uPYLdy9mVaWXZt/XUU2rX8/PFL4qv8G42bKi875JTTvOQrDL2luVeLdUvqXPDBuDWW71fDidOVJ5nbEyuPVElqyZlGydqXExWJQrKazx7eFgs1XjuOWDuXO9lxv394quosxfwGnvPZkWvCIjbVmFZwOio2nX8vPqq99/sEZg9e0Tm4/79wB/+IHWzh5G8MvaTk+6/7+wEvva1qZzcRYtEZ/2971Ve1n45ZDLhkmejSlZNyjZO1MAirmkSCguaUWIFqcJVXs9bpZSoXZnLskQpUV1FxaI4FKuwFpGy3kXOasKE9qbMnBnu+um0Ze3aVfn0u9Wci/LhjLKYGKuqUhRY0IwoamESS3fuFCtLZJfqlmdAFovA2Wd7Z0wmmAXAmVox+fG/V2I3+rDCTKMk9PVNJWvGWXMujmRRO5e4UHC/T14JuER+mKxKFLUw49TXXadWL8SySifp02ngP//n4OdPkCaIQKQb63EVkr2E99Qp4OWXgTVrogtCZswo/XccxcRMbShNZGMgQhREmAl7r8SDapzBz/Llwc9vkN3Plffj81HAHqxIbDAyNAS0tYkaG24Jp7p89JHIJ9m0Cdi7V4xCxLFsllVVySQmq1J1JvZ2Sfp+MiMj8Z/zrbdKz9/UFDyoMax88W8TLEwihR7kMYDlWPblNH7xCyNN81RePj4qfX3i59NPi5GKuIKAzs6p/OKkvu2oPjFHhPy5reywi2RF9Qlp4pw2mQCoSoGuSNm9VJI3xgupA4O4srsD69ebbolZLCZGtUyl/2YgQt5MlFs0WeJRNgCKogKqrNZW4MwzzQRBMbmttRcPHb4OixZ5J1A2kvZ28XRPmxbP+ZI+GEm1gcmqFJ6JcosmSzzaAVB5B2/XxXAW6TJZUOHEiboOQgDgb746D/v31/Wgj5KREZG7IVsjL4yw9fmIgmAgQu5MlFs0VeJRNgA6dUqMhkgW6CI1k0jhPeSw5h+WYNmyqfptcX0b7+pSL1wbl2PHKuNh3VRicSKdGIiQOxPlFk2VeJQNgObPF18RH3gg3PnsjUo2blTf1jUqM2YAM2caO/3kx+mrefRUbHxn5+P+7d9G24Z//MdoV8QA4mm/7LLg1y8fENS1zRH3myGTGIiQO9nlqTrrTps4JyAf2OgqINbaKnJdtm4FnnlGz22G9dFHwJ/+ZOz0w8hiBfbgeVTm/1iW6MD/z/+Jtg2/+U20U0H2UthXXw028lI+ILh7t6hrp2MahfvNkEkMRMhdtZ1gnVuT1vI5geg28fAyc+ZUHZD334/33AmURzcW4oBrEGKzLJEr0dYWY8M0WroUOHhwKuf5jjuC39b//J9in5tVqypj4+HhYNMo3G+GTGIgQu5MlFs0VeKxWgCk2/Aw8OCDorfI5+M5p5uErEQ7grkV0zFeLr444sZEZHAQuPlmYNcuMWqxeXPw29qxA/inf/L+u2WpT6OYGowkAhiIkB8T5RZNnDOdFmXX41yisXmzKKFpYr+Ypiago0N+//mIqey6++KLETYkYs88A1xzjfcUyN/9nZi102FoCLj/fvm8EVODkUQA64iQjHqvrLp7txjnptgV0YRrsDPRG97FqbVVJMymUvriYtlagPaqGaD03CysRkGwoBmRrD17gGuv9f/aaO94S9pZACykPBNVG1UmAxw/rue2VAIJt5p+uZyYEWUQQioYiFBjCTp60t8vpkdk6fyaGiXde9B87nPAa6/pu70yk0hhGFksxAHpXJF6lkqJmcm/+zvgf/wPfbeZzYpN9Kq9NVhZlXRgZVVqHEFLQdqFE2Tl88mp+VGNriBkxgyxPCPCIAQQG96diyEsAdeGAiLWHR4GRkf13qbs8tt0WqQQXXed+MkghKLGQIRqV5hSkNUKJ5Rbvlysv9y7V19GYdJ99BHw05/Gdrp54NpQp0ce0R8ExL38VlfBNapvDESoNoUtBanyiWwvF0inxRF1+c0GpbJ6plHo7rjjXH7LfWtIFgMRqk1hS0GqfCI7a5eoBDB2lmB5GU2OdZeYBPAectgPrg2NStzLb7lvDalgIEK1qVCQu5xX4CBTxCydFhWonMsFVAKYbBbo6wOOHBEVrXp7xc9nn42veFqNcNtjhrzNmlX671xObF1kb2PkFGUtQDfct4ZUnWG6AZRwSUyhV6lI6hU42FVcV6zwXg3z3HNThRVsdgBTKHivoGltFQGMM9Ovo6P0Mq+/Dnz3u3L3oc6dBFfEqUilgP/wH4AXXgCOHp16WwIiQHn44dLZw2w23uW3KoOV5W8LalBWgo2OjloArNHRUdNNaUx9fZaVzVqW+OwQRzYrfm+yTalUaZvcjlTKsnI5y5qYqH575fcxl/O/j3Ybytth/87tuhMTljU4aFm9vZa1d2/lORv4KCJlFZGyrkKf6aZoOWRenjqOwUH/l3Frq2V1dVV/C+jW2yvX/t7eeNtF8VLpvxFDewJjIGKQV4fv19lGbWJCrgNXbaMzSBgclPvkVglg3C7Lo+QoImW9i5zVhAnTTQl95HKWtWtX9E+53ZEn7a06OCjXfmcgRfVHpf9mQTOqVCyK9Hav8VWV6kg67dsnUu+raW8HHnss+rFomWkrO2sv6reZnRygs5CZAR0YxCvoMN0MZZdcInbUnT9/6mWwcSOwbVt05xwcFOdK2lvV/vjwmr009fFB8VLpv5kjQpWSOskru2Klu1tfEOIXbNiVn/yu65W1p5Od45Lc7xTSarWWyK9/LQKR228XL5WBAWDnzmjOZXfkS5Yk663qfKusWSP2dSxPv4o7cZZqAwMRqiTb4cddHUl2xYquCqhuG2/I7iAGqBdNK2cv+3VuOtLcDPzFX5T+bv584E9/0rc5iUG1XEvkxAmgq0vPbX3lK+47DTs7cgB4+WW524v6rer2VnF7+cadOEu1gYEIVZLt8OOsjgRUX7Hi/KoYlteUil0IQWYHsTCf/l1dwH/7b+L/y0dkyn9XLALLlgU/V0JYANpwzHQzEuHb3xajCm5xsB2E+E3JlIvyrer1VrFX7nR1AYsXJ2fRHSUPAxGqFGeHr8Jvya3bmG/QpcfVCiGkUmL58PLl3rdXLIr6IUHk88B99039u3xMvbwAw/vvBztPAj2EDXgeVzVsTRHnWyudFi+x8pfwwIB62tGxiOI7mZohP/oR80GoishTZ0PgqhmDgixRjbNt1VashFl6HDbtP+wqGeftlq/ocVuO0dxsfqmIxuNLGDTdBKPHrl3eL03ZhWPlRyYjVo3rXsrLFTLkRaX/5ogIuevsFNMPXmPDJid5OzvdvyraX7nCTquEyZHRsUrmBz8QXzVPnAA2bKg+/j42FvxcCVSrCau6rF8vXspuL9GgaUfHj4vZO5UUJxmyBY537xY/OTVDbrh8l/zZ0xuFAjAyIpbGOtcoJo2Opceyy4QHB0unTaqdm6ToWMLb1FTbK5lTKVGct62tNNbetUtsIBfmdgG5FCcZPT0icJKlOxCi5OLyXdInnRYTzOvXi0DEltRPlAcfDL+e0c6RqRbMlOfIhF0l0+AmAQxr2vyuloMQQLxMr722NB0omxUJrGFvVybFSVZ7u9rlVXK9qXFw0zvyd9ddwMqVpUEIIDrcpG2j2d8vihfI8Jt+GRgQy2Hd+BVCiHs5cx3SsfldvewnWJ6TXCjIv7z9OGPxsFRXytvj79z0jpwYiJC33bv9N2azLHOfKMWimEJ59lnx89Qpkc8iy2s9o53j4VWTo7W18uuc3ZY//EH+/FSiiCaswi48j/Bfk5M72RyO7vulI262Bw9V6AyEqD5EHog88sgjWLBgAWbMmIFLL70Uv/71r6M+JelQLAK33lr9ciY+Ufr7RS7G0qViwnzpUv+plHK5nPvSY5lKqDNnijFtO/hYv14ENUuXAg88EOTeNDwLwLV4Fn1YabopoWUylRs2J5WO2iL2ivogo1AcQCRbpIHIzp07sWHDBmzevBlvvPEGPvOZz+CKK67A0aNHozwt6bB/v3zxgTg/UewRi/Kgo3zqyI9XfWmZHI/hYZGHYgdCPT1q56YK/y++gD1YZboZWpw4IQbMkiyV8o7Fg7AX2KmOjMRdD5GSK9JA5KGHHsKaNWtw00034VOf+hQee+wxnHnmmfj7v//7KE9LOqgEF3F9oujYu6WryztLTvY+b97MpFRNLABfwP/GVUhQrlEISZsWKh+pUN3r5dQpcdnbbxc/T51yv1xnJ3DwoFhItmOHWO3j1yadgRDVvsgCkVOnTuE3v/kNljlKTzc1NWHZsmX41a9+5Xqd8fFxjI2NlRxkiGxw0d4e3ydK2FUp2exU2XQ3tfQVrb0duPde060ILfXxf3uQRxOYvahLLidSvMqTSVtbgfvvF7OL1dx1F3DmmWL28Qc/ED/PPFP83o29B+QNNwCPPz61IXQ5ywKuvlq8nZmwSgCAqKqqFQoFC4D1y1/+suT3GzdutC655BLX62zevNmC+JJUcrCyqgGyJRx3746vTb29wUpVylaDte9zeTXZJB59fcHLbCb0aPSKqldeaVlr1+q5LfulPjFhWV1dltXaWvr3akWGN270v/2NG6u/Xd0KDKfTau3Qobw4se7qsuROpbIqompEkEDko48+skZHR08fQ0ND0neEImCXeQ/zaaSTbD3ptrbSf5eXf/fjVdo+SUcmM/VpWu05qqHjWvSabkJdHF1dlS/n8sv4xebj45UBQ/mRTovLVWMHAfm8++1EvWNEmJ0eKByVQCSyqZm2tjak02kcKdv468iRIzj77LNdrzN9+nQ0NzeXHGSQVxZae7so8bh1a7ztsdcKeqXo25PPhYKYrO7tFT8PHJCvnmTfZ9UCCXE6fhzYvl0sXW5tFc+Fvee6l9WrY2laGIdRQ1NjCbZ4sfgpsyGd2+r7Rx+tPmVSLIrLVZNOi7etVwKvXzvC8sprt4uqJakEUqOLLBCZNm0aLrroIrz88sunfzc5OYmXX34Zn//856M6LenmzEKzO/bDh0WRs7jZawUB/yy8adPEZPV114mfquUj7fvc3R2uvVFav35q6fL69SLQ8ArcM5notl/VZAJptCHZbawVb70lflZLqbIs99X377wjdx7ZywVtRxhBgzAyI9IS7xs2bMCNN96Iiy++GJdccgl6enrw4Ycf4qabborytKSbnYWWBHFtxpdOq9evLjdjBvDRR3ra42d4GPje97z/fvw48NOfRt+OEJpQxC6swgrs0VLUrJHZ1VcXLZK7/Msvl+5nI3s92cuF2UMyKJXgJykfbY0s0kDkmmuuwcjICO677z68//77uPDCC/Hiiy9i7ty5UZ6Wap290Z7bzrpA9d13dejvV9vNy834uJ62NIAmiL1mepDHAJaHLvPe6DZv9l9C6+Ssw5fN+hdTdvrmN+UuJ7sYTeeiNRPBDwXH3XcpWfr73Uc74txgz55cDvrWyOWAyy8Hnn5aa7MahY7dd+tFdzcwdy4wZw5w443AoUPBX5a6lW8+7cXelLpQcG+7zIbYqoJuoE36qPTf3GuGksMru2x4WBQeWL9efMJEObGrWjQtnwf27hWHMzl21iy189bLTm0azAO/ptp517ffLlKdLr8c+P73TbeqlOxogmxql84BTdm8dhZVSwYGIpQMMgFAT4/4mrNggd6Ud+cGetu3yxVNa28H+vrEV9bLLxeHMzlWdgLddt99QVpel7h6RijvnO30KNkpl6ipTKV4LUbLZiv3kNTBRPBDwXFqhpJBdiwVmPok0fEJ5jYVJGPHDlFC0mbntRQKYu+ZT3wC+PrXgcnJ6reVzQI33yzKzze4Y2jFXBxt6ByRTAZ44gnvl/apU+IlY2qLozBTKdXSv3Rze3vncnrz2smdSv8dabIqkbSBAfnLWpb4NMznRdJq0E+yMLkgzq92QYMZ25/+xCDkYw9jXUMHIQCwc6cYYPMybRrw2GNitjJuYUcT4l6AF0deO4XHEREyr1gUnxBBvuINDopPFtVPGjuDTjV4KP86GCaYaWqSGzFpABaAY8jgbBxp6EAkkwGOHJF7+c6dK1ZmR6m1VewobONoAsniiAjVlv37g48zDwwA/+W/qK+yCbKBXvnXwbC7ATMIASCCEAD4Jp5o6CAEEIHFgw+KvRn9gpH9+6MPQgBRtDed5mgCRYvJqmRemMX8PT3BajgHOWd5Zl3Y3YCpRBMYmAGiBsh55+l/+aqwV5V0dIQrUkwkg4EImRe0kpHXp6JMDWfZc3Z3e+9Zw2pIWqQ+Ph7BrWgCa24D1WNpncW/ynFVCcWNgQiZV23Rfzn7cn71RKptYCFTaCCbBf76r73PEWVvEIdUShzf+lYiepy5GMESaNxwpA54xdIyL19Z5U99VEtqibwwECHz/Bb9u8lmxSe0DK9Ri2qFBixLrGZZtmxqczln/ZJiURytrXLtSKL580WP893vAn/7t6ZbA4DFzJz8YmmZl6+M7m7g3/89+GbVRDowEKFk8Kp4lMsBu3dXflIuXy53u36jFl7ntIOL8mxAe7z8rrtEULJsWemSgqjdcotY5pvN6rm9p58Wj8G3vpWYTfFYzKySVyzt9/KVLew7d274zaqJwuLyXUoW2YpHOjewcJ7T3tSjUNByd3zZX11lv8I2NQF33il2KdNR0aq3F/iLvwBWrgx3OxpYAIaQxUIcbPiVM+W6urwL79pFgfftE/8+4wzg/vvlb5t7rVBUuHyXapdsxSN7bHrFisqOXDXbznnOffvCBSHNzcDYmNxls1nRxokJ4Jprql9+clJMo7z3np6ymnPmiK/BCZAC8CTWMAhx8eST7st53eroqYxmtLZOzTByFIRM4tQM1a4oNrAIuxJGNgjZtGlqMn7VKmDjRvlz7NoVrG1OmYz4aapOuIu3sdh0ExJpeLgyT8Rrf0iV/SBPnBCzi7q3biJSxRERqm26azjHtRLm8stL27h1K3DRRWJaaHzc/7o6ZlPvuAN4//3wt6MR80O8HT5cup1RPq/nZQBMpT5xpQyZwkCEap/ODSzsdZFeuScy2tpEoqtf7kr5/uP9/SJptFoQYjvrLLHcIUgbMxkx1r99u/p1I3IE7diP+t+TffZs4K/+CnjtNbXrvfVWsB0JZOjauokoKE7NEDk510WqsstR/uAH3kEIUJm74jXO7sdOMPXa49ztb7YnnhDnP3hQ/nwRsT4+1uKRhsgPOXkSeOcdtes0N4sE1CiL+FYru0MUJQYiROXs3JP2drXrWRbwN38jRjbcpFJi1Ytz/DvIfjVNTcCjj3rnx/T1icNtKXRfnzh/sShWzSTAVmxEH8yv3AlDpYCYalrOGWfom4aphsWCyQROzRC56ewEvvY10ZkfOyZ/vYEB779NTgLbtolea+tW8bsg+9VMTgJ/+Zdi5ObgQe/8GL/cmTAbDWpyBO1Yi0fRhxVG2xFE+cbJbW3ArbeKpbY6zZgRrFRNefva2+We7lovFky1iYEIkZdp04DHHxfTJoC+r6Xf/S7w2c+K6ZWgX0FlMgz9cmcMf/X9d8yo2SAEEJ28c6X2yIhIufFbvZ1KiYBFJf776KPg7WtpAT7/eeCKK4BvflPErtXK7pSnLsmSLf9D5IZTM0R+vJYIh7V2rfj0njMn2PWrbexnV7p69lnxs/wyQc+ryQx8hF1YhaswtW5UZXojCcoDjhMnqq/efuQR+aqnYY2OAi++CKxfL4IQu2SMV1pR0E3u+vtFIu3Spe67IRBVw0CEqJrOTjEF0tWlrxcZGQEefFAs1w3KK8OwBnoG+4OnB/nTO+5ms/EVeZ02rfTfcQQH9t6CH3yg93abJD7FCwUxK/itb+ktu+OVZ11t92AiJ5Z4bwQcNw3P/sRN4tult1cURdu/X+So9PRUXsb+2mv3OM8+K4KUBHh50yDSl3dgZEQM8Bw6FP05Z80SK1HOPlscq1dHuyollZoKAKI8T7U2ZLPA228Dv/xl+I8De5cFr/ujsssC1R+WeKcpbnWgs1mR6MjqRXKCrGyJk0yRifJiEQnKSrz8U4fRf0JUuY/rIf7gAzE6kM0C//W/Rh8cWJa5AMTZhqEhEYToKLtTLc/aOWDH/WzIDwOReub1LZ6lFNUEWdkSl0xGfLWX6cHtnuH++xP1FfVH/zQPG/7RTJw3PKy2SVw90JWnLHs7XBJM1TAQqVd+3+JZSlFN0j9JVXvwBx6Iph0BFJHC//PjyzBhuiENRNdgmOztJGjwjRKKyar1SmXclPwF/STt7BRrKKN0/Hi0tx+xNCwsAV+DcbAL/wZdolvO3g3Ba7WT7vNR/WIgUq84blqq2nJWv8sXi/6fuOXsT+Bdu0SgMDgI3HZbyDtQv3aXLeMlNdkssHu3+Okl7BJdN87dEHQsCZZ9i6q+lakGWAk2OjpqAbBGR0dNN6X2DA5alhj38D8GB023NHp9fZaVzZbe72xW/F728pmM3ONpH87bnpiwrO5utes30FEErCJS1lXoM92UiiOVMt+GakcmI15uExPi7ZzPW1Z7e+llcjnvl3sUby/V88m+RVXfymSOSv+NGNoTGAORECYmxDvU65M0lRKfFhMTplsarb4+98cglRKH2yed1+Vle4Z8vvT2yj85dfWQ2az/c1xDRxEp613krCZMmG5KyfHzn4vOfccO0bmbeqjzee9Y2O2lbAclvb3iZ9Rv8zDnk32Lqr6VySwGIiTY79zyd2+jvHPtYMzr0708GKt2ednDHmXy+uQMezifv6jOYej4EgZNN8H1peF8Ok083Hv3qr2Ua4XsW3R8vD7vfz1T6b+ZI1LPvMqThymlmHTOCeTt29USdsMu03Vm50VZe8T5/NnPcWur/vPokMkotW0ezOcspVLiabv6avGSsHMQoqr2X60tuZz4/3rMPZfNqX/00fq8/yQwEKl3dnnywUFRgXNwUJQ6rMcgpLy0+fr1ctezE3ZVEnerZefprj3S3V36/C1fPhVwtbaKn2Hde69aUq6MO+4Ajh4VG/1JOAzzaz3tkuk9PZXV8cvfTps2RdcO50vq6FG569Ra7rlse995R+/tUbKwjkgj8NuFtV6EKcFuL8+VXabb1QU8+WRltdqenqkAT/YT8d57gcce897r3a6TffvtU8sPvKrlZjLidoKOwvyn/wRcdJEYCtBl8WJRdn7LFt+LWQCKSON/4zJ951Z09dVAX1/lKozhYfG3ri5xd5xl0fftUyvLksmIhVT2qIsf50tq3z6526+1mh2y7ZV9Sdfa/aePxTBVFBhzREhK0NwOrxwRmQTfatl5squWfv5zy+rq8j5feS6PTDJtkCQG+3719elNbvC6bx6HqRyRTMayWlvlL2+v1JB96a1bN/UyqZa/3NoqHjbnS6pec8+r3S/nkU57/61W7389Y7IqNRbZTr/8k8tv1YzXJ2M+L7csQPYTtqnJ+2/layBlMvuamy3rE59QfzxUelXZo7VVednztejVdnqV48Ybg798du2qfvnyTnJiQi3+9Htp1nruedgk4Fq///WKgQg1lt5e9U8vv0IHbl9Zy7+OyRQvCPMJW/6V2LLUA67Zs+Uu19ws2hokoNN8JGXVjGwHmMuJFS0yl3eW7FFd0OX30lSp2RH3sl5ZKqvcy9+KUdZIoeAYiFBjke1Au7vlP4Gd1aG8egqZr2F9fZY1f756L9fcLNYsOqkGXKoB0Lp1xnr1IpDIOiIyx6ZNcpfr7VV/ybrVGwwaTAQpBhZn4KJS90/lrUxmMBChxhLVBHrQr63lZL8ylx+zZ5f2ElGPWPhNE0V8FIFEVlaVOWQDEWdQIRtTOoOXMIIUAzNRxTTux4Wiwzoi1Fii2vTi/vv1FC+QXXtZ7uRJsVzDXjtabZexsCYno7ndKqxUE1ZhF55HbS4p7+hQ3/wtzp1rq23EDYiNuJ2rhexFaOUv/0JB/L4/oq2BuKNvg4ohMAqMIyKkJKpNL8J+PQs7kuEcdYmrvGeM5UO3zLg/rlNpf4ici41UEknjXAWjOg2kayAwiHpdHdSIOCJC9c9tC86wxdu8vgZWU+3rWdiRDOeoS1zlPdvaor19h//vo7+M7Vy6lA+0qRYx1j2I50d1I27ZaqdRVDGN83Gh5GAgQrWnvIKqs/ylXbztuuvET5XpGNWS7G5j7m78Pl1lFQqllVS3bQt2O7Kuuy7a23dIQjVVVW4BxvLlwNNPi2qrmzYBe/f6x8Fx7cCgOt2hGrjo1og7UzS6lGWpfPLGa2xsDC0tLRgdHUVzc7Pp5lASeFVQtTv4oJ9U+/aJgEZWkPO5VUSV1d4OjIxM/TudriwB6tXG1lb1iqvl54vAJFIYRhYLcQCTqI2vuJs2AZdfPlVZ1eZV7Pbhh6u/PIpFMbpQKIiHvL1ddMLl5wiqWBRxeqHg/hKwi/ceODBVLVbmrTA4GG3BZvtxOXy4tJot1Qal/jvyiaIQmCNCJaKcvFZdGiube1K+/nF8fOrfilVHlY/29tIdemXyPlIpcb2IEywmkbKKSEmtlMlkRNEw+2GTXeIZxeGWDqRje/qoV6io5LAwT4N04PJdqk9hii/ouu1Nm+SLF7j1LvPniwDEDkx271auPip97Njh3xavntOrdorG4ygyUkFIc7N4iGwTE2I1tEo5dq9j/nzL2rzZsmbNkr9O+UtLR2ysI5CRoZLLXa9VXCk+DESoPkVZZED310Cv3qX8yGbF1/2ursreNezIhFuv6RyNKS+0ZvdKEdcrEcXLstLFy+yOL8iCJr/D7kwnJkRA4vd0eT39YWPjKAf53IqRqRQo07EIjRoXAxGqT1GOiFiWvq+BKnu2OG+7vJfYsSNYDyvbe3n1Srr3nPE4VMq5ZzL6VhRnMu5PpdeeMX5Pf9jYOKqXtK6pnqSWhKfk4/Jdqk/VlsHKrmLxoitdv9r6RyfLEj/zefFzyRKRmXf4cLBkUZU1jl4rjOxVPlEVTvvYPMgvuzh+fOqhcjNrltzt3HsvcOSI+1O5ciXQ1yeebie/p192RcqRI+65xVGsUNFZjCzoIjQiJTEERoFxRIQq+CVe6pq8Dvs1MMgmfICYLqm22V75EeUOYLrnQkKMiOg6ZEYWVJ5+lW3s3UYkdI+ImCxGRuSk0n9z+S7Vnv5+4BvfEF+TnTIZ4IknzBcaUF0KHIQ9WrFzp1jvGdUaR+fa0pdeEo/9yZNTf29qKi0Nn8uJGifXX++5vNgCUEQaM/HvmMA0bU1tbQX+9V/dR07Kl6jqZI9AAP6jNm4rvqstrQXEQyrb7qQsvSVS6b85NUO1qTwIAUStjCg3wpAVxZ4w5b2QPV+wcmW0Y+f22PzMmcD/+l+lQQgwFYTk81OVbOfM8a1xkgJwBor4j/il1qauW/fx7UdYkdOroK9MsVvnLJz98Dhr3Xm59lrvdpe3p1CQux9RFSMjCiTy8ZkQODVDFWpl7DmKPWHK9z6PK5NQ9jG3a6TcdpvU/Vnb2qvtoWlvF6cPutJD5qGslgCqso19+VTLxo3+D6/s7rhtbWamqIjKcdUM1a+oV87oJFu7Q7a3dS69iHOP9r175aMBhehhYu9gSUe3eXO4YMS++6odqMxDKVvrI8gqmiCxtezqcJnbCvqYEPlhIEL1K8paIrJUsxmdtTvcvq7LVli1g6soKmDZlcI2bRLH3r1T28rqqB4m0RtOTFSWNlG9WdW7L/NQqgQKQeJk3bvjOtsV5PGJq8Aa1TcGIlS/TI+IhP2q6FVlSraYWhRTU17BhkrJUZVoARDVW12CuLAzWip3X/ahlB0Qsu+Oal081dha9i1QPkAlO0VVCzOflHysI0L1K+paIn50FGhwK8ygsve57j3a+/uBq68Wib7lPvhA7jZUNH38kdPTc3rX5OKe/pKNhXfurJ746UXl7ss+lPv2yZ378OFg29hHtTtud7fIH+7tncojXr68MtnWSffLi0gGAxGqLUE+6XUoFsWyDMuq/Jv9O+dyCFWyxdR0VsAqFoE1a9TaGdSnPz11TgdruIDUyhX4/tJ+XH+9iE02bABuvjnc6WTuvu6VI3agoFoXTzW2lg1c5s8vjXkHBsRS4aVLcfqxXrCgNH6OosAaUVUxjNAExqkZ8hT3RhhxTQlVyz/R2Q7ZOYeIjyJS1rvInd57RsdCI5m7L/tQ7t0bbBsi1X1dotwdVzbvw/TMJ9UP5ohQY4hzfWESkmQtS+/mfJs2GQ9CnIeOSqtBckRkHso4dqONandclbwP3Xs/UuNijgg1hjg3wlCdyI+KqakpGSELuKnsPeN3etm7r/JQ6tqGyE9nJ3DwYGVeh9ttq7RHJe8jyS8vqmMxBEaBcUSEEiNpXxV1VO7atk3fiMasWWJb2whGRNy+9QOVp6t2970G0FQeyqQV+ZJpT5DBvLhnPqn+cK8Zoih4bSritomIDHsfl6D7xJw6BTz6KPDOO8CiRcCttwLTfPZu6e8XCbfOr8fle8W4aW11X1XjZD8G998vdg3+wQ+k7gIAWACOIYOzcQSTKL3/XV3Ak0+WNjmXE9/Kly+Xf/jc7no2K779d3aGfypM82t/0P1nav0xIbOU+u8oIqEDBw5YX//6160FCxZYM2bMsM4//3zrvvvus8bHx5VuhyMilDi6viqGrUeiev2gpTiXL5/62r1jh38NcdXCGx8fkx8fV6HPM2/BPn13t/g5ODhVUb7a6ES9F+iSKT2fpME8agzGk1V/9rOfWatXr7Z+/vOfW++88441MDBgzZkzx7rzzjuVboeBCCVS2PH5sD2j6vVlS3F69VK7don7efXVctfZvNmy0mml8zhXzrjdDbfOtvwUbnFYLRboCrLaptpLIY5kWyIn44GIm61bt1oLFy5Uug4DEao7YXvGINeXXZPpdSgGFWGOL2GwYoBJdjDHrVOtteWoKgNdqi8F5n1QnBK5amZ0dBStra2+lxkfH8fY2FjJQVRXwpauDHL9sNWnghZpC+DRTYdLVor41ZErZ3ev3/gG8PLL4rq1VKBLtXCv6ktBZVUOUZxiCUTefvttbN++Hd/85jd9L7dlyxa0tLScPnK5XBzNI4pP2J4xyPWjXlKs0acun1eSEFmts3Vz/DiwbJmoGvrWW3LXMf0QBSncG+SlEOeKdyJZSoHI3XffjVQq5Xu8+eabJdcpFAr4yle+gpUrV2JNlXLS99xzD0ZHR08fQ0ND6veISLdi0X+DDhWyPd6cOe7nDFLPpFoN8STw2CMozEhFoQBs3gxkMma2JlIRZKArKaVtiMI6Q+XCd955J1avXu17mfPPP//0/x86dAhLly7FZZddhieeeKLq7U+fPh3Tp09XaRJRtKqt+1S1ZImoQlUouP89lRLLZW+8sfQy9jmXLxf/Xyi4f31OpcTfnT2rXaVqxQrxd+f1yv9tgk+lrDCdqGWVBiBud93jtLELMrphx5cqLwWiRIoqUWV4eNhavHixde2111oTAVPSmaxKRkWx7rOvz7vwl19GpvOcQZdAeGUrbtzofnu6j1RK3Pf586UzJqstPZU9urqSnagZNKmWq2EoqYwXNCsUCujo6MB5552HZ555BmnH142zzz5b+nZY0IyMKRZFkoHXeLn9dfPAAfmv03Y2otdbLpMRP48fr37OgYHKkRq70pdfhS6v3/f3A3fcUToKY19eB2fRN5VKZPCuI6eitxdYtSq5Bbrsl1u10Q23l5vboJ3zpUBkgvGCZk899ZQFwPVQwRERMkb3uk+ZWh7t7WrnVKlZXq1Ymtt17JEbHSMlVYYfqtXO2L278uFRWVWclOW5fsKMbiSt9DxRIuuIBMFAhIzRvdtu2FoesucMMp3kdx1nQOIMmPJ5UeZUpr3d3b49Y7W4ye3vbW2W9dxzoohra6v3qZNYsMwPa31QvTA+NaMLp2bImKAbdHh59lng+uvDtsr/nEGmk2SuM38+8PTTwNGjldM8QecTPuY1W2XP5HzrW8C2bd5/37NH/NS5BZBp3OOF6oFK/x1bQTOimlJtyavquk/Z5R9tbcHPGWQNqMx1hodFT1hefCLknvEytTMeesj/7/m8SDnZs0fES07ZbO0FIQBrfVDjYSBC5CZkJ1tBNrB59NHg5xwYkGuLcw1o2AJrnZ2BowCZGMgvV9YZV7FqKFHtYiBC5CVEJ1tBNrBZuTLYOfv7xfVlOEdndFTFChgF6Cqrbt8ORxKIahNzRIiq0TlpL7vWUuWc1fI8bH45IiHyPIKSTcOpRjZNh4jio9J/MxAhipvubESVHr2vr3KkwqtQR8TZnjIxUFOT9/RMhDESEYXEZFWiJNM9hyA7x5HPuwcUOqegFMjMVm3YIP5fR5oOESUTAxEiU3Rtpieb53Heed7nMJTtWS0G2rq1vlbEEFElTs0QmaBzM71qcxxOYTbsi5BztmrOHPE7Z9kSgLU1iGoJc0SIkqxaFa8gX/VlN2RJeJUv3ZsdR43Fx4jcMRAhSqooNtOzufXius8RoSjisyjVWtBEFCcmqxIlVZDqp7LsPI/ubv/LhTlHRGSqrObz+jYDDssOmsqfykJB/L6/30y7iGoRAxGiOIWtZFpNOg3MnRvtOSIQZXymW60FTURJx0CEKE46Kpkm4RyaRR2f6VRLQRNRLWAgQhQn3ZvpmTqHZrUUO9VS0ERUCxiIEMVJ92Z6ps6hWS3FTrUUNBHVAgYiRHGLo5KpoWqpQdVS7FRLQRNRLeDyXSJT4ihCUWOFLmT3BDTN0PY8RDWDdUSIqGbVSuxUK0ETkQkMRIiIYlArQRNR3FT67zNiahMRUd2xN1ImouCYrEpERETGMBAhIiIiYxiIEBERkTEMRIiIiMgYBiJERERkDAMRIiIiMoaBCBERERnDQISIiIiMYSBCRERExjAQISIiImMYiBAREZExDESIiIjIGAYiREREZAwDESIiIjKGgQgREREZw0CEiIiIjDnDdAOIqAEVi8D+/cDhw8C8ecCSJUA6bbpVRGQAAxEiild/P7BuHTA8PPW7bBZ4+GGgs9Ncu4jICE7NEFF8+vuBFStKgxAAKBTE7/v7zbSLiIxhIEJE8SgWxUiIZVX+zf5dPi8uR0QNg4EIEcVj//7KkRAnywKGhsTliKhhMBAhongcPqz3ckRUFxiIEFE85s3TezkiqgsMRIgoHkuWiNUxqZT731MpIJcTlyOihsFAhIjikU6LJbpAZTBi/7unh/VEiBoMAxEiik9nJ7BnDzB/funvs1nxe9YRIWo4LGhGRPHq7ASWL2dlVSICwECEiExIp4GODtOtIKIE4NQMERERGcNAhIiIiIxhIEJERETGMBAhIiIiYxiIEBERkTEMRIiIiMgYBiJERERkDAMRIiIiMoaBCBERERnDQISIiIiMYSBCRERExjAQISIiImMYiBAREZExDESIiIjImMgDkfHxcVx44YVIpVL47W9/G/XpiIiIqIZEHojcddddOOecc6I+DREREdWgSAORn/3sZ/jFL36Bbdu2RXkaIiIiqlFnRHXDR44cwZo1a/DCCy/gzDPPlLrO+Pg4xsfHT/97bGwsquYRERFRAkQyImJZFlavXo1bbrkFF198sfT1tmzZgpaWltNHLpeLonlERESUEEqByN13341UKuV7vPnmm9i+fTtOnjyJe+65R6kx99xzD0ZHR08fQ0NDStcnIiKi2pKyLMuSvfDIyAiOHz/ue5nzzz8fq1atwk9+8hOkUqnTvy8Wi0in07jhhhvwzDPPSJ1vbGwMLS0tGB0dRXNzs2wziYiIyCCV/lspEJH13nvvleR3HDp0CFdccQX27NmDSy+9FNlsVup2GIgQERHVHpX+O5Jk1XPPPbfk37NmzQIALFq0SDoIISIiovrHyqpERERkTGTLd50WLFiACGaAiIiIqMZxRISIiIiMYSBCRERExjAQISIiImMYiBAREZExDESIiIjIGAYiREREZAwDESIiIjKGgQgREREZw0CEiIiIjGEgQkRERMYwECEiIiJjGIgQERGRMQxEiIiIyBgGIkRERGQMAxEiIiIyhoEIERERGcNAhIiIiIxhIEJERETGMBAhIiIiYxiIEBERkTEMRIiIiMgYBiJERERkDAMRIiIiMoaBCBERERnDQISIiIiMYSBCRERExjAQISIiImMYiBAREZExDESIiIjImDNMN8CPZVkAgLGxMcMtISIiIll2v233434SHYicPHkSAJDL5Qy3hIiIiFSdPHkSLS0tvpdJWTLhiiGTk5M4dOgQZs+ejVQqZbo5gY2NjSGXy2FoaAjNzc2mm9PQ+FwkB5+L5OBzkRz18lxYloWTJ0/inHPOQVOTfxZIokdEmpqakM1mTTdDm+bm5pp+YdUTPhfJweciOfhcJEc9PBfVRkJsTFYlIiIiYxiIEBERkTEMRGIwffp0bN68GdOnTzfdlIbH5yI5+FwkB5+L5GjE5yLRyapERERU3zgiQkRERMYwECEiIiJjGIgQERGRMQxEiIiIyBgGIoaMj4/jwgsvRCqVwm9/+1vTzWk4Bw8exM0334yFCxdi5syZWLRoETZv3oxTp06ZblrDeOSRR7BgwQLMmDEDl156KX7961+bblLD2bJlCz772c9i9uzZmDNnDq688kr88Y9/NN0sAvCd73wHqVQK+XzedFMix0DEkLvuugvnnHOO6WY0rDfffBOTk5N4/PHH8fvf/x7d3d147LHHcO+995puWkPYuXMnNmzYgM2bN+ONN97AZz7zGVxxxRU4evSo6aY1lFdeeQVr167Fa6+9hpdeegl//vOf8eUvfxkffvih6aY1tNdffx2PP/44Pv3pT5tuSjwsit0///M/W5/85Cet3//+9xYA61/+5V9MN4ksy9q6dau1cOFC081oCJdccom1du3a0/8uFovWOeecY23ZssVgq+jo0aMWAOuVV14x3ZSGdfLkSWvx4sXWSy+9ZH3pS1+y1q1bZ7pJkeOISMyOHDmCNWvW4B/+4R9w5plnmm4OOYyOjqK1tdV0M+reqVOn8Jvf/AbLli07/bumpiYsW7YMv/rVrwy2jEZHRwGA7wOD1q5di69+9asl7496l+hN7+qNZVlYvXo1brnlFlx88cU4ePCg6SbRx95++21s374d27ZtM92Uunfs2DEUi0XMnTu35Pdz587Fm2++aahVNDk5iXw+jy984Qu44IILTDenIT333HN444038Prrr5tuSqw4IqLB3XffjVQq5Xu8+eab2L59O06ePIl77rnHdJPrluxz4VQoFPCVr3wFK1euxJo1awy1nMistWvX4ne/+x2ee+45001pSENDQ1i3bh1+/OMfY8aMGaabEyuWeNdgZGQEx48f973M+eefj1WrVuEnP/kJUqnU6d8Xi0Wk02nccMMNeOaZZ6Juat2TfS6mTZsGADh06BA6Ojrwuc99Dk8//TSamhibR+3UqVM488wzsWfPHlx55ZWnf3/jjTfi3/7t3zAwMGCucQ3qtttuw8DAAF599VUsXLjQdHMa0gsvvICrrroK6XT69O+KxSJSqRSampowPj5e8rd6wkAkRu+99x7GxsZO//vQoUO44oorsGfPHlx66aXIZrMGW9d4CoUCli5diosuugg7duyo2zd5El166aW45JJLsH37dgBiWuDcc8/Fbbfdhrvvvttw6xqHZVm4/fbb8fzzz2Pfvn1YvHix6SY1rJMnT+Ldd98t+d1NN92ET37yk/j2t79d19NlzBGJ0bnnnlvy71mzZgEAFi1axCAkZoVCAR0dHTjvvPOwbds2jIyMnP7b2WefbbBljWHDhg248cYbcfHFF+OSSy5BT08PPvzwQ9x0002mm9ZQ1q5di97eXgwMDGD27Nl4//33AQAtLS2YOXOm4dY1ltmzZ1cEG2eddRYymUxdByEAAxFqUC+99BLefvttvP322xVBIAcJo3fNNddgZGQE9913H95//31ceOGFePHFFysSWClaP/zhDwEAHR0dJb9/6qmnsHr16vgbRA2JUzNERERkDDPziIiIyBgGIkRERGQMAxEiIiIyhoEIERERGcNAhIiIiIxhIEJERETGMBAhIiIiYxiIEBERkTEMRIiIiMgYBiJERERkDAMRIiIiMoaBCBERERnzfwEcRI6JuLGUtwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install seaborn\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "list_points_xy = [[np.random.normal(), np.random.normal()] for x in range(0, 1000)]\n",
    "x, y = zip(*list_points_xy)\n",
    "\n",
    "x_activated = [x for x in list_points_xy if x[0] >= 0]\n",
    "x_not_activated = [x for x in list_points_xy if x[0] < 0]\n",
    "\n",
    "plt.axis([-5, 5, -5, 5])\n",
    "plt.scatter(*zip(*x_activated), color = 'blue')\n",
    "plt.scatter(*zip(*x_not_activated), color = 'red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0ef78",
   "metadata": {},
   "source": [
    "#### Entropia relativa\n",
    "\n",
    "Dada a função chamada de *sigmoide*, definida por $\\sigma(X) = \\frac{1}{e^{-X}+1}$, retiramos as identidades:\n",
    "\n",
    "* $Q_J(d=1|x) = \\sigma({xJ_x+yJ_y+J})$\n",
    "\n",
    "    que pode ser demonstrado conforme:\n",
    "\n",
    "    $  \\displaystyle \\begin{align}\n",
    "      Q_j(d=1|x) &= \\frac{e^{(xJ_x+yJ_y+J)}}{1+e^{xJ_x+yJ_y+J}} \\\\\n",
    "      &=  \\frac{e^{(xJ_x+yJ_y+J)d}}{1+e^{xJ_x+yJ_y+J}}\\cdot \\frac{e^{-(xJ_x+yJ_y+J)}}{e^{-(xJ_x+yJ_y+J)}} \\\\\n",
    "      &= \\frac{1}{e^{-(xJ_x+yJ_y+J)}+1}\\\\\n",
    "      &= \\sigma(xJ_x+yJ_y+J)\\\\\n",
    "      &\n",
    "      \\end{align}$\n",
    "\n",
    "* $Q_j(d=0|x) = 1-\\sigma({xJ_x+yJ_y+J})$\n",
    "\n",
    "    com a demonstração:\n",
    "\n",
    "    $  \\begin{align}\n",
    "    1-\\sigma({xJ_x+yJ_y+J}) &= 1-\\frac{1}{e^{-(xJ_x+yJ_y+J)}+1} \\\\\n",
    "    &= \\frac{e^{-(xJ_x+yJ_y+J)}+1-1}{e^{-(xJ_x+yJ_y+J)}+1}\\\\\n",
    "    &= \\frac{e^{-(xJ_x+yJ_y+J)}}{e^{-(xJ_x+yJ_y+J)}+1}\\cdot \\frac{e^{(xJ_x+yJ_y+J)}}{e^{(xJ_x+yJ_y+J)}} \\\\\n",
    "    &= \\frac{1}{e^{(xJ_x+yJ_y+J)}+1}=Q_J(d=0|x)\n",
    "      \\end{align} $\n",
    "\n",
    "\n",
    "Com isso, se pode calcular a entropia relativa para esse modelo, conforme a fórmula introduzida no capítulo anterior:\n",
    "\n",
    "$ \\displaystyle D_{KL} = \\sum_{x,d}P(x,d)\\log{\\frac{P(x,d)}{Q_J(x,d)}}=\\sum_{x,d}P(x,d)\\log{\\frac{P(d|x)P(x)}{Q_J(d|x)P(x)}}=\\sum_{x,d}P(x,d)\\log{\\frac{P(d|x)}{Q_J(d|x)}}$\n",
    "\n",
    "onde, utilizando as propriedades do logaritmo, obtemos a expressão:\n",
    "$= \\sum_{x,d}(P(x,d)\\log{P(d|x)} - P(x,d)\\log{Q_J(d|x)})$.\n",
    "\n",
    "Podemos observar que somente a primeira parte depende de $J$, e decidimos ignorar o segundo termo por hora, visto que, quando aplicamos o gradiente para calcular o erro posteriormente, o termo sem dependência em $J$ desaparece. \n",
    "\n",
    "Assim, podemos aproximar o primeiro termo utilizando a lei dos grandes números (fazendo a mesma observação do capítulo anterior, que considera o somatório como um valor esperado) e aproximando o valor esperado pela média das variáveis aleatórias, isto é:\n",
    "\n",
    "$\\displaystyle \\sum_{x,d} -P(x,d)\\log{Q_J(d|x)} \\approx \\sum_i ^ N \\frac{1}{N} \\log{Q_J(d_i|x_i)}=-\\left[ d_i \\log{\\sigma(x_i J_x + y_i J_y + J)} +(1-d_i)\\log{1-\\sigma(x_i J_x + y_i J_y + J)}\\right]$.\n",
    "\n",
    "#### Valor esperado de classificação\n",
    "\n",
    "O cálculo feito a seguir no livro tem como objetivo calcular o valor esperado de uma label considerando uma entrada específica. Isto é, relembrando as definições inicias, queremos descobrir, dado um ponto $x_i$ do *dataset*, qual é o valor esperado com o qual este ponto será classificado. Assim, se faz:\n",
    "\n",
    "$\\displaystyle < d >_{J,x_i} = \\sum_{labels} d \\ Q_J(d|x_i) = 0 \\cdot \\ Q_J(0|x_i) + 1 \\cdot Q_J(1|x_i) = \\sigma(x_i J_x + y_i J_y + J)$\n",
    "\n",
    "Isto é, calculamos a média das _labels_ somando a probabilidade de cada label multiplicada pelo valor da mesma, verificando que a média segue uma sigmoide. Como esse é o valor esperado da classificação, chama-se de _output_ da máquina.\n",
    "\n",
    "#### Cross entropy\n",
    "\n",
    "Os autores então introduzem o conceito de _cross entropy_, dado por $L(x,d) = -(d \\log(x) + (1-d)\\log(1-x))$ para este modelo. Se olharmos com cuidado, a cross entropy é somente uma função que representa a parte interior da aproximação feita para a divergência de Kullback-Leibler, onde $x$ é a função $ \\sigma(x_i J_x + y_i J_y + J)$. De fato, a notação que se utiliza no livro $L(<d>_{J, x_i}, d_i)$ nada mais é do que calcular $L(\\sigma(x_i J_x + y_i J_y + J), d_i)$, dado o valor esperado das labels que calculamos acima.\n",
    "\n",
    "A escrita nessa parte é um pouco confusa, mas o conceito principal é que nós __calculamos__ a cross entropy para cada tripla $\\{J, J_x, J_y\\}$, e minimizamos a entropia relativa $D_{KL}$. O que o autor diz no livro é que minimizar a entropia relativa é equivalente a minimizar _cada termo_ da soma, ou seja, minimizar a cross entropy para cada entrada.\n",
    "\n",
    "#### Rede com 4 classificações\n",
    "\n",
    "Na parte seguinte, os autores introduzem um sistema com 4 classificações distintas ao invés de duas. Os cálculos são essencialmente os mesmos, porém com a introdução de uma matriz $\\mathbb{J}$ que representa os $J_i$'s (que eram somente $J_x$ e $J_y$ anteriormente) e um vetor $\\textbf{J}$. Assim, com um vetor de labels e o vetor de entrada, temos a equação $-\\textbf{d} \\cdot (\\mathbb{J}\\textbf{x} + \\textbf{J})$. Escrevendo explicitamente cada termo, podemos representar (escolhi representar os termos da matriz com índice superscrito, pois acho que fica mais claro assim):\n",
    "\n",
    "$\\textbf{x} = \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y\n",
    "\\end{bmatrix}, \n",
    "\\textbf{J} = \\begin{bmatrix}\n",
    "    J_1 \\\\\n",
    "    J_2 \\\\\n",
    "    J_3 \\\\\n",
    "    J_4\n",
    "    \\end{bmatrix},\n",
    "\\mathbb{J} = \\begin{bmatrix}\n",
    "    J_{x}^1 & J_{y}^1 \\\\\n",
    "    J_{x}^2 & J_{y}^2 \\\\\n",
    "    J_{x}^3 & J_{y}^3 \\\\\n",
    "    J_{x}^4 & J_{y}^4\n",
    "    \\end{bmatrix},\n",
    "\\textbf{d} = \\begin{bmatrix}\n",
    "    d_1 \\\\\n",
    "    d_2 \\\\\n",
    "    d_3 \\\\\n",
    "    d_4\n",
    "    \\end{bmatrix}\n",
    "    $\n",
    "    \n",
    "o que nos leva, escrevendo o vetor __d__ como um vetor linha ($\\textbf{d}^T$) para explicitar o produto escalar, a:\n",
    "$\\begin{bmatrix}\n",
    "    d_1 & d_2 & d_3 & d_4\n",
    "    \\end{bmatrix} \\left(\\begin{bmatrix}\n",
    "    J_{x}^1 & J_{y}^1 \\\\\n",
    "    J_{x}^2 & J_{y}^2 \\\\\n",
    "    J_{x}^3 & J_{y}^3 \\\\\n",
    "    J_{x}^4 & J_{y}^4\n",
    "    \\end{bmatrix}\\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y\n",
    "\\end{bmatrix} +  \\begin{bmatrix}\n",
    "    J_1 \\\\\n",
    "    J_2 \\\\\n",
    "    J_3 \\\\\n",
    "    J_4\n",
    "    \\end{bmatrix}\\right)$\n",
    "    \n",
    "Isto é, realizamos uma multiplicação de uma matriz por um vetor primeiro ($\\mathbb{J}\\textbf{x}$), resultando em um vetor de 4 dimensões, seguida da soma por um vetor de mesmo tamanho ($\\mathbb{J}\\textbf{x}+\\textbf{J}$), e por fim realizamos um produto escalar entre o resultado das últimas operações e o vetor de labels ($\\textbf{d}\\cdot(\\mathbb{J}\\textbf{x}+\\textbf{J})$).\n",
    "\n",
    "As próximas contas são análogas às feitas anteriormente, mas dessa vez com 4 termos para se considerar. É interessante notar o surgimento da função __softmax__, dada por $\\sigma_I (X) = \\frac{e^{X_I}}{\\sum_J e^{X_J}}$, que é uma extensão da sigmoide. Isso acontece devido ao denominador conter a função de partição Z, que dessa vez possui $n$ termos ao invés de 2, também mão usando mais o 0 como estado, pois nesse caso não estaria contando ele e não compensaria no outro como no caso biário.\n",
    "\n",
    "#### Rede com classificações infinitas\n",
    "\n",
    "Consideramos agora um dataset em que se estabelece $d \\in (-\\infty, \\infty)$, isto é, com as labels variando na reta. Nesse caso, os autores consideram outro hamiltoniano, $H_{J,x}(d) = \\frac{1}{2}\\left(d-(\\mathbb{J}x+J)\\right)^2$, dado com o intuito de aparecer uma distribuição gaussiana posteriormente. Aqui é importante notar duas coisas:\n",
    "\n",
    "* A escolha dos hamiltonianos é essencialmente arbitrária, assim como a distribuição de 'chute' $Q_J$; a escolha inicial foi motivada pelas constantes de acoplamento, mas a expressão tomar forma com produtos escalares pelas labels feita anteriormente já não aparece aqui. Nesse caso, a label é subtraída das outras quantidades visando uma expressão mais fácil de lidar quando utilizarmos a gaussiana. Teoricamente, qualquer função bem comportada o suficiente (segundo Vapnik, 1998, ela precisa ser absolutamente contínua) que dependa dos parâmetros J pode ser utilizada. A questão é selecionar bem a função para que o tempo de treinamento não seja muito longo ou até maior que a expectativa de vida do universo, e acredito que ninguém lendo essas notas tem tempo pra isso. \n",
    "* Note que o $\\mathbb{J}$ é necessariamente uma matriz $1 \\times 2$, do tipo $\\begin{bmatrix} J_x & J_y \\end{bmatrix}$, isto é, um vetor linha, pois o produto matricial precisa ser um número real para ser somado com J e d. Aqui podemos considerar a motivação por essa escolha, visto que anteriormente aumentamos o número de dimensões de d e J para abranger mais classificações. O que motiva essa escolha, a princípio, é a capacidade de um número real de representar essas infinitas classificações na variável de label d, o que também justifica $(\\mathbb{J}x+J)$ ser um número real.\n",
    "\n",
    "Assim, podemos expressar a função de chute como:\n",
    "\n",
    "$\\displaystyle Q_J(d|x) = \\frac{e^{\\frac{-1}{2}\\left(d-a\\right)^2}}{Z} = \\frac{e^{\\frac{-1}{2}\\left(d-a\\right)^2}}{\\int_{-\\infty} ^\\infty e^{\\frac{-1}{2}\\left(x-a\\right)^2} dx}$,\n",
    "\n",
    "onde $a = \\mathbb{J}x+J$ e a variável $d$ no denominador foi substituída por x por conveniência. O importante é notar que a integral do denominador é uma gaussiana comum, somente _shiftada_ $a$ unidades para a direita.\n",
    "\n",
    "A gaussiana possui integral conhecida e bem definida; quando integramos em toda a reta (de $-\\infty$ a $\\infty$), a integral é igual a $\\sqrt{2\\pi}$. De fato, se fizermos uma substituição $x-a=u$, com $\\frac{du}{dx} = 1$, temos que o denominador se torna:\n",
    "\n",
    "$\\displaystyle Z = \\int_{-\\infty} ^\\infty e^{\\frac{-1}{2}\\left(u\\right)^2} du = \\sqrt{2\\pi} \\Rightarrow Q_J(d|x) = \\frac{e^{\\frac{-1}{2}\\left(d-a\\right)^2}}{\\sqrt{2\\pi}} $.\n",
    "\n",
    "O valor esperado das labels vem naturalmente, considerando que a gaussiana, para cada ponto de dados $x_i$, é centrada em $x_i=(\\mathbb{J}x_i+J)$, e portanto $<d>_{J, x_i} = (\\mathbb{J}x+J)$. A função de erro $L(<d>_{J,x_i}, d_i)$ é, considerando a definição anterior, o termo interior da entropia relativa (um logaritmo de $Q_J$):\n",
    "\n",
    "$\\displaystyle L(<d>_{J,x_i}, d_i)= -\\log(e^{\\frac{-1}{2}(d-(\\mathbb{J}x+J))^2}) = -\\frac{-1}{2}(d-(\\mathbb{J}x+J))^2 = \\frac{1}{2}(d-(<d>_{J,x_i}))^2$,\n",
    "\n",
    "o que o autor nota se tratar do erro quadrático médio, concluindo então, finalmente, que acabamos de \"fazer\" uma regressão linear por meio de uma rede neural simples. Isso faz sentido quando consideramos o que significa tomar as labels como um valor real. Fazendo isso, é como se nós \"classificássemos pontualmente\" cada ponto de input, obtendo uma classificação na reta como output.\n",
    "\n",
    "#### Regressão com graus de liberdade adicionais \n",
    "\n",
    "Nessa parte, se introduz um modelo para labels com __d__ indo de $0$ a $\\infty$. Esse modelo se utiliza dos resultados para a ReLu (Rectified Linear unit), introduzida dessa forma no trabalho seminal de V. Nair, 2010, que utiliza o conceito de *binomial units* de Teh & Hinton, 2001, permitindo expressar cada unidade com mais informação. Isso se dá por meio da ideia de utilizar os mesmos pesos e bias' mas com 'infinitas' (*N* grande) cópias distintas. No livro, essa ideia é feita com o hamiltoniano:\n",
    "\n",
    "$ \\displaystyle H_{J,x}(\\{ h_{bit}^{(u)}\\}_{u\\in I \\subset \\mathbb{N}}=-\\sum_{u=1}^N(\\mathbb{J}\\textbf{x}+J+0.5-u)h_{bit}^{(u)}$\n",
    "\n",
    "que é exatamente igual ao hamiltoniano da primeira classificação que fizemos, mas com um *offset* no bias de $(0.5-u)$. Isto é, o bias é alterado pela soma de $0.5-u$ unidades $(-0.5, -1.5, -2.5, ...)$, o que dá mais liberdade ao sistema. \n",
    "\n",
    "Também podemos perceber que o termo que seria o label __d__ aparece como $h_{bit}^u$. Assim se explicita o aparecimento uma camada que não necessariamente é uma label (que mais tarde se diz ser uma hidden layer). Podemos escrever a distribuição de chute como:\n",
    "\n",
    "$\\displaystyle Q_J(\\{h_{bit}^u\\}|\\textbf{x}) = \\prod_{u=1}^N  \\begin{bmatrix} \\sigma(\\mathbb{J}\\textbf{x}+0.5-u), \\ \\ \\text{se } h_{bit}^ u = 1 \\\\ 1-\\sigma(\\mathbb{J}\\textbf{x}+0.5-u), \\ \\ \\text{se } h_{bit}^ u = 0 \\end{bmatrix}$\n",
    "\n",
    "E, dado\n",
    "\n",
    "$ \\displaystyle h = \\sum_{u=1}^N h_{bits}^u$,\n",
    "\n",
    "e a função de erro dada pelo hamiltoniano auxiliar $H_h(d) = \\frac{1}{2} (d-h)^2$\n",
    "\n",
    "$\\displaystyle Q(d|h) = \\frac{e^{-\\frac{1}{2}(d-h)^2}}{\\sqrt{2\\pi}}$,\n",
    "\n",
    "podemos definir a probabilidade condicional:\n",
    "\n",
    "$ \\displaystyle Q_J(d|\\textbf{x}) = Q(d|h)Q_J(\\{h_{bit}^u|\\textbf{x}\\}) = Q\\left(d|h = \\sum_{u=1}^N h_{bits}^u \\right)Q_J(\\{h_{bit}^u\\}|\\textbf{x})$\n",
    "\n",
    "##### aqui está faltando elaborar, é bem complicado de entender\n",
    "\n",
    "No livro, se calcula a função erro com as mesmas aproximações anteriores usando a lei dos grandes números. A parte importante de se considerar é que $<h>_{J,x_i} \\approx \\sigma_{ReLu} (\\mathbb{J}\\textbf{x_i}+J)$ e que $Q(d_i|h=<h>_{J,x_i}) \\approx \\sum_{\\{h_{bit}^u\\}} Q(d_i|h = \\sum^N h_{bits}) Q_J(\\{h_{bit}^u\\}|x_i)$. Isto é, a saída dos neurônios de entrada é dada pela *função de ativação* ReLu, que é dada pelo valor esperado de $h$. Isso nos permitirá extrapolar o conceito para a próxima seção."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834160d",
   "metadata": {},
   "source": [
    "### Subseção 3.1.2\n",
    "Consideramos agora uma rede neural profunda (*deep neural network*). Sabendo que a saída dos neurônios de entrada é $<h>_{J,x_i}$, que é uma ReLu, podemos considerar um vetor $\\vec{h_i}$ para cada hidden layer adicional. Por exemplo, uma camada com três neurônios que recebe *n* entradas do dataset possui uma saída dada por:\n",
    "\n",
    "$<\\vec{h_i}>_{J,x_i}= \\ <\\begin{bmatrix} \n",
    "    {h_{i}^{1}} & {h_{i}^{2}}  & {h_{i}^{3}}\n",
    "\\end{bmatrix}>_{J,\\textbf{x}} $\n",
    "\n",
    "Assim, para cada camada teremos um hamiltoniano, i.e., uma lista de hamiltonianos $({H_{J_1, \\textbf{x}}^1}(\\textbf{h}_1), {H_{J_2, \\textbf{h}_1}^2}(\\textbf{h}_2), ..., {H_{J_N, \\textbf{h}_{N-1}}^N}(\\textbf{d}))$. Portanto, a probabilidade $Q_J(d|x)$ de chute, que é baseada nos hamiltonianos, será uma composição de todas as probabilidades individuais relativas aos $Q_{J_i}$'s. \n",
    "\n",
    "No texto, o modelo utilizado é somente imposto, sem maiores explicações. Tentarei justificar a escolha de maneira intuitiva. O modelo em questão é dado por:\n",
    "\n",
    "$Q_J(\\textbf{d}|\\textbf{x}) = \\sum_{\\textbf{h}_1,...,\\textbf{h}_{N-1}}Q_{J_N}(\\textbf{d}|\\textbf{h}_{N-1})...Q_{J_2}(\\textbf{h}_2|\\textbf{h}_{1})Q_{J_1}(\\textbf{h}_1|\\textbf{x})$\n",
    "\n",
    "O termo interior da soma é dado por probabilidades condicionais. Isto é, o último termo do produto ${Q_J}_{1}(\\textbf{h}_1|\\textbf{x})$ é a probabilidade de se obter a saída $\\textbf{h_1}$ na primeira camada dado um ponto $\\textbf{x}$ na entrada; o segundo termo $Q_{J_2}(\\textbf{h}_2|\\textbf{h}_{1})$ nos diz a probabilidade de se obter um valor $\\textbf{h}_2$ na segunda camada, dado que obtivemos uma saída $\\textbf{h}_{1}$ na primeira hidden layer, e assim sucessivamente. \n",
    "\n",
    "Faz sentido, portanto, considerar a probabilidade inicial $Q_J(\\textbf{d}|\\textbf{x})$ (probabilidade de se obter uma label de saída da rede $\\textbf{d}$ dado um ponto inicial $\\textbf{x}$) como o a probabilidade de se obter uma certa saída dada a primeira entrada vezes uma certa saída dada a segunda entrada, ..., ou seja, como um produto de probabilidades de cada passo da rede. \n",
    "\n",
    "O somatório aparece para contabilizar todos os possíveis valores dos vetores $\\textbf{h}_i$, de forma a cobrir todas as possibilidades que levam um ponto inicial $\\textbf{x}$ a um label $\\textbf{d}$.\n",
    "\n",
    "O conceito que apresentamos acima é escancarado pela aproximação feita no texto, substituindo o somatório somente por um valor esperado:\n",
    "\n",
    "$Q_J(\\textbf{d}|\\textbf{x}) \\approx {Q_J}_N(\\textbf{d}|<\\textbf{h}_{N-1}>)$\n",
    "\n",
    "que é uma aproximação pelo fato de tomarmos uma média do que esperamos como saída na última hidden layer, ao invés de considerar cada termo. A aproximação é boa, entretando, porque o valor médio dessa última camada depende de todos as outras camadas. De fato, utilizando a equação obtida anteriormente para o valor esperado $<h>_{J,x_i} \\approx \\sigma_{ReLu} (\\mathbb{J}\\textbf{x_i}+J)$, podemos escrever\n",
    "\n",
    "$<\\textbf{h}_{N-1}> = \\sigma_{ReLu}^{N-1}(\\mathbb{J}_{N-1}<\\textbf{h}_{N-2}>+\\textbf{J}_{N-1}), ..., \\sigma_{ReLu}^{1}(\\mathbb{J}_{1}<\\textbf{x}>+\\textbf{J}_{1}),$\n",
    "\n",
    "o que é muito interessante e simplifica muito o problema, se tornando somente uma questão de calcular de forma recorrente os valores esperados para cada hidden layer. \n",
    "\n",
    "O livro faz, então, outra observação interessante. A estrutura que vemos na equação acima explicita o processo de aprendizado, que é composto dos passos iterativos: fazer uma transformação linear $(\\mathbb{J}_{i}<\\textbf{h}_{i-1}>+\\textbf{J}_{i})$, seguida de uma transformação não linear pela função de ativação que, nesse caso, é uma ReLu. Isso torna o aprendizado possível, pois somente uma transformação linear em cada neurônio não é o suficiente para o processo de aprendizado captar características complexas do conjunto de dados em análise; utilizar uma versão sem não-linearização seria o equivalente a fazer uma regressão linear simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19e1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
