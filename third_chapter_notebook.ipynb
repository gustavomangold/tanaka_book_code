{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7cc88a1",
   "metadata": {},
   "source": [
    "# Capítulo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbea500",
   "metadata": {},
   "source": [
    "### Secão 3.1\n",
    "Neste capítulo, se introduz o conceito de redes neurais utilizando mecânica estatística. A função de 'chute' $Q_J$ se constrói por meio da distribuição de Boltzmann e com o uso de um hamiltoniano escrito na forma $H_{J,x}(d) = -(xJ_x+yJ_y+J)d$. Para uma rede com somente dois *labels* de classificação, com aprendizado supervisionado, temos:\n",
    "\n",
    "$Q_j(d|x) = \\frac{e^{(xJ_x+yJ_y+J)d}}{1+e^{xJ_x+yJ_y+J}}$\n",
    "\n",
    "onde as constantes $J$ são os parâmetros, que chamamos de *constantes de acoplamento*. Dada a função chamada de *sigmoide*, definida por $\\sigma(X) = \\frac{1}{e^{-X}+1}$, retiramos as identidades:\n",
    "\n",
    "* $Q_j(d=1|x) = \\frac{e^{(xJ_x+yJ_y+J)}}{1+e^{xJ_x+yJ_y+J}}=\\frac{e^{(xJ_x+yJ_y+J)d}}{1+e^{xJ_x+yJ_y+J}}\\cdot \\frac{e^{-(xJ_x+yJ_y+J)}}{e^{-(xJ_x+yJ_y+J)}}=\\frac{1}{e^{-(xJ_x+yJ_y+J)}+1} = \\sigma(xJ_x+yJ_y+J) $\n",
    "\n",
    "\n",
    "* $Q_j(d=0|x) = 1-\\sigma({xJ_x+yJ_y+J})$\n",
    "\n",
    "onde a segunda identidade pode ser demonstrada conforme:\n",
    "\n",
    "$1-\\sigma({xJ_x+yJ_y+J})=1-\\frac{1}{e^{-(xJ_x+yJ_y+J)}+1} = \\frac{e^{-(xJ_x+yJ_y+J)}+1-1}{e^{-(xJ_x+yJ_y+J)}+1}=\\frac{e^{-(xJ_x+yJ_y+J)}}{e^{-(xJ_x+yJ_y+J)}+1}\\cdot \\frac{e^{(xJ_x+yJ_y+J)}}{e^{(xJ_x+yJ_y+J)}}=\\frac{1}{e^{(xJ_x+yJ_y+J)}+1}=Q_J(d=0|x)$\n",
    "\n",
    "Com isso, se pode calcular a entropia relativa para esse modelo, conforme a fórmula introduzida no capítulo anterior:\n",
    "\n",
    "$D_{KL} = \\sum_{x,d}P(x,d)\\log{\\frac{P(x,d)}{Q_J(x,d)}}=\\sum_{x,d}P(x,d)\\log{\\frac{P(d|x)P(x)}{Q_J(d|x)P(x)}}=\\sum_{x,d}P(x,d)\\log{\\frac{P(d|x)}{Q_J(d|x)}}$\n",
    "\n",
    "onde, utilizando as propriedades do logaritmo, obtemos a expressão:\n",
    "$= \\sum_{x,d}(P(x,d)\\log{P(d|x)} - P(x,d)\\log{Q_J(d|x)})$.\n",
    "\n",
    "Podemos observar que somente a primeira parte depende de $J$, e decidimos ignorar o segundo termo por hora, visto que, quando aplicamos o gradiente para calcular o erro posteriormente, o termo sem dependência em $J$ desaparece. Assim, podemos aproximar o primeiro termo utilizando a lei dos grandes números (fazendo a mesma observação do capítulo anterior, que considera o somatório como um valor esperado) e aproximando o valor esperado pela média das variáveis aleatórias, isto é:\n",
    "\n",
    "$\\sum_{x,d} -P(x,d)\\log{Q_J(d|x)} \\approx \\sum_i ^ N \\frac{1}{N} \\log{Q_J(d_i|x_i)}=-\\left[ d_i \\log{\\sigma(x_i J_x + y_i J_y + J)} +(1-d_i)\\log{1-\\sigma(x_i J_x + y_i J_y + J)}\\right]$.\n",
    "\n",
    "O cálculo feito a seguir no livro tem como objetivo calcular o valor esperado de uma label considerando uma entrada específica. Isto é, relembrando as definições inicias, queremos descobrir, dado um ponto $x_i$ do *dataset*, qual é o valor esperado com o qual este ponto será classificado. Assim, se faz:\n",
    "\n",
    "$< d >_{J,x_i} = \\sum_{labels} d \\ Q_J(d|x_i) = 0 \\cdot d \\ Q_J(0|x_i) + 1 \\cdot Q_J(1|x_i) = \\sigma(x_i J_x + y_i J_y + J)$\n",
    "\n",
    "Isto é, calculamos a média das _labels_ somando a probabilidade de cada _label_ multiplicada pelo valor da mesma, verificando que a média segue uma sigmoide. Como esse é o valor esperado da classificação, chama-se de _output_ da máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ed9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
